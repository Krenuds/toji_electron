# Phase 3: STT Implementation Roadmap

## Visual Timeline

```text
Week 1: Whisper Setup        Week 2: Audio Capture      Week 3: Integration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                            â”‚                          â”‚
â”œâ”€ Day 1-2: Build whisper   â”œâ”€ Day 6-7: Audio capture  â”œâ”€ Day 11: IPC setup
â”‚  â€¢ Clone repo              â”‚  â€¢ node-microphone       â”‚  â€¢ Handlers
â”‚  â€¢ Build binaries          â”‚  â€¢ Test mic access       â”‚  â€¢ API surface
â”‚  â€¢ Download models          â”‚  â€¢ Stream to whisper    â”‚
â”‚                            â”‚                          â”œâ”€ Day 12: Routing
â”œâ”€ Day 3-4: Electron bundle  â”œâ”€ Day 8-9: Wake word     â”‚  â€¢ Project context
â”‚  â€¢ Copy to resources/      â”‚  â€¢ Parse output          â”‚  â€¢ Discord voice
â”‚  â€¢ electron-builder.yml    â”‚  â€¢ Activation logic      â”‚  â€¢ Fallbacks
â”‚  â€¢ Test packaging          â”‚  â€¢ Buffer management     â”‚
â”‚                            â”‚                          â”œâ”€ Day 13-14: UI
â”œâ”€ Day 5: Service stub       â”œâ”€ Day 10: VAD & silence  â”‚  â€¢ Button component
â”‚  â€¢ stt-service.ts          â”‚  â€¢ Silero VAD setup      â”‚  â€¢ Status display
â”‚  â€¢ Process spawning        â”‚  â€¢ Silence detection     â”‚  â€¢ Hotkey setup
â”‚  â€¢ Path resolution         â”‚  â€¢ Finalization logic    â”‚  â€¢ Testing
â”‚                            â”‚                          â”‚
âœ“ Milestone: Binary runs     âœ“ Milestone: Audio works  âœ“ Milestone: E2E works
```

---

## Phase 3.1: Whisper Setup (Days 1-5)

### Day 1: Clone & Build

```bash
# Morning: Clone whisper.cpp
git clone https://github.com/ggml-org/whisper.cpp.git

# Afternoon: Build
cmake -B build -DWHISPER_SDL2=ON
cmake --build build -j --config Release

# Evening: Test locally
./build/bin/whisper-stream -m ./models/ggml-base.en.bin
```

**Success:** Real-time mic transcription works locally.

---

### Day 2: Models & Testing

```bash
# Morning: Download models
sh ./models/download-ggml-model.sh base.en
sh ./models/download-ggml-model.sh tiny.en

# Afternoon: Test VAD
./models/download-vad-model.sh silero-v5.1.2
./build/bin/whisper-stream --vad ...

# Evening: Benchmark performance
./build/bin/whisper-bench -m models/ggml-base.en.bin
```

**Success:** All models work, VAD tested, performance acceptable.

---

### Day 3-4: Electron Bundle

```text
Project structure:
resources/
  whisper/
    bin/
      whisper-stream (or .exe on Windows)
    models/
      ggml-base.en.bin
      ggml-silero-v5.1.2.bin
```

```yaml
# electron-builder.yml
extraResources:
  - from: 'resources/whisper'
    to: 'whisper'
```

```bash
# Test packaging
npm run build
npm run build:win  # or :mac, :linux
```

**Success:** Binary and models bundle correctly, accessible at runtime.

---

### Day 5: Service Stub

```typescript
// src/main/services/stt-service.ts
export class STTService {
  private whisperProcess: ChildProcess | null = null;
  private modelPath: string;
  private binaryPath: string;

  constructor(config: ConfigProvider) {
    this.binaryPath = this.resolveWhisperBinary();
    this.modelPath = this.resolveModel('base.en');
  }

  async startListening(): Promise<void> {
    // Spawn whisper-stream
    // Pipe audio
    // Parse output
  }

  async stopListening(): Promise<void> {
    // Kill process
    // Clear buffers
  }
}
```

**Success:** Service spawns process, can start/stop.

---

## Phase 3.2: Audio Capture (Days 6-10)

### Day 6-7: Microphone Capture

```bash
npm install node-microphone
```

```typescript
import { spawn } from 'child_process';
import mic from 'node-microphone';

const microphone = mic({
  rate: 16000,      // 16kHz required by Whisper
  channels: 1,       // Mono
  bitwidth: 16,      // 16-bit PCM
  encoding: 'signed-integer'
});

const whisper = spawn(whisperBinaryPath, [
  '-m', modelPath,
  '--vad',
  '-'  // Read from stdin
]);

// Pipe audio to whisper
microphone.startRecording();
microphone.pipe(whisper.stdin);

// Read transcription
whisper.stdout.on('data', (chunk) => {
  console.log('Transcription:', chunk.toString());
});
```

**Success:** Audio flows from mic â†’ whisper â†’ transcription output.

---

### Day 8-9: Wake Word Detection

```typescript
class STTService {
  private isActivelyListening = false;

  onTranscriptionChunk(text: string) {
    if (!this.isActivelyListening && text.toLowerCase().includes('listen')) {
      this.activateFullRecording();
      log('ðŸŽ¤ Wake word detected! Now recording...');
    }
  }

  activateFullRecording() {
    this.isActivelyListening = true;
    this.audioBuffer = [];
    this.startSilenceTimer();
  }
}
```

**Success:** Saying "listen" activates recording.

---

### Day 10: VAD & Silence Detection

```typescript
class STTService {
  private silenceTimeout: NodeJS.Timeout | null = null;
  private audioBuffer: Buffer[] = [];

  onTranscriptionChunk(text: string) {
    if (!this.isActivelyListening) return;

    // Add to buffer
    this.audioBuffer.push(Buffer.from(text));

    // Reset silence timer
    if (this.silenceTimeout) {
      clearTimeout(this.silenceTimeout);
    }

    // Set new silence timer
    this.silenceTimeout = setTimeout(() => {
      this.finalizeTranscription();
    }, 1500); // 1.5 seconds
  }

  async finalizeTranscription() {
    const fullText = this.audioBuffer.join(' ');
    log('âœ… Transcription complete:', fullText);

    await this.sendToDestination(fullText);
    this.reset();
  }
}
```

**Success:** 1.5s of silence triggers finalization.

---

## Phase 3.3: Integration (Days 11-14)

### Day 11: IPC Handlers

```typescript
// src/main/handlers/stt.handlers.ts
export function setupSTTHandlers(sttService: STTService) {
  ipcMain.handle('stt:start', () => sttService.startListening());
  ipcMain.handle('stt:stop', () => sttService.stopListening());
  ipcMain.handle('stt:toggle', () => sttService.toggle());
  ipcMain.handle('stt:status', () => sttService.getStatus());

  // Events from main â†’ renderer
  sttService.on('listening', () => {
    BrowserWindow.getAllWindows()[0]?.webContents.send('stt:listening');
  });

  sttService.on('transcription', (text: string) => {
    BrowserWindow.getAllWindows()[0]?.webContents.send('stt:result', text);
  });
}
```

```typescript
// src/preload/api/stt.api.ts
export const sttAPI = {
  start: () => ipcRenderer.invoke('stt:start'),
  stop: () => ipcRenderer.invoke('stt:stop'),
  toggle: () => ipcRenderer.invoke('stt:toggle'),
  onListening: (cb: () => void) => ipcRenderer.on('stt:listening', cb),
  onResult: (cb: (text: string) => void) => ipcRenderer.on('stt:result', (_, text) => cb(text))
};
```

**Success:** Renderer can control STT, receives events.

---

### Day 12: Routing Logic

```typescript
class STTService {
  async sendToDestination(text: string) {
    // Priority 1: Discord voice
    if (this.discordService.isVoiceConnected()) {
      const channelId = this.discordService.getCurrentVoiceChannelId();
      await this.discordService.sendMessage(channelId, text);
      log('ðŸ“¤ Sent to Discord voice channel');
      return;
    }

    // Priority 2: Active project
    const activeProject = this.configProvider.getActiveProject();
    if (activeProject) {
      await this.tojiService.sendMessage(activeProject.id, text);
      log('ðŸ“¤ Sent to active project:', activeProject.name);
      return;
    }

    // Priority 3: General prompt
    await this.tojiService.sendGeneralPrompt(text);
    log('ðŸ“¤ Sent to general Toji prompt');
  }
}
```

**Success:** Transcriptions route to correct destination.

---

### Day 13-14: UI Implementation

```typescript
// src/renderer/src/hooks/useSTT.ts
export const useSTT = () => {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState<string>('');

  useEffect(() => {
    window.api.stt.onListening(() => setIsListening(true));
    window.api.stt.onStopped(() => setIsListening(false));
    window.api.stt.onResult((text) => setTranscript(text));
  }, []);

  const toggle = async () => {
    await window.api.stt.toggle();
  };

  return { isListening, transcript, toggle };
};
```

```typescript
// src/renderer/src/components/STTButton.tsx
export const STTButton: React.FC = () => {
  const { isListening, transcript, toggle } = useSTT();

  return (
    <Tooltip label={isListening ? 'Stop listening' : 'Start voice input (Ctrl+Shift+L)'}>
      <IconButton
        aria-label="Voice input"
        icon={isListening ? <Icon as={FaMicrophoneSlash} /> : <Icon as={FaMicrophone} />}
        colorScheme={isListening ? 'red' : 'blue'}
        onClick={toggle}
        isLoading={isListening}
        loadingText="Listening..."
      />
      {transcript && (
        <Text fontSize="xs" color="gray.500" mt={1}>
          {transcript.substring(0, 50)}...
        </Text>
      )}
    </Tooltip>
  );
};
```

```typescript
// Add to Header.tsx
<HStack>
  <STTButton />
  <ProjectSelector />
  {/* other header items */}
</HStack>
```

**Success:** UI shows status, button works, hotkey registered.

---

## Testing Checklist

### Unit Tests (Day 14)

```typescript
describe('STTService', () => {
  it('should start listening on button press', async () => { });
  it('should detect wake word', () => { });
  it('should finalize on silence', () => { });
  it('should route to active project', async () => { });
  it('should fallback to general prompt', async () => { });
});
```

### Integration Tests (Day 14)

```typescript
describe('STT E2E', () => {
  it('should capture audio from microphone', async () => { });
  it('should spawn whisper process', async () => { });
  it('should parse transcription output', () => { });
  it('should send to Discord', async () => { });
});
```

### Manual Testing (Day 14)

- [ ] Click button â†’ mic activates
- [ ] Speak â†’ see transcription in logs
- [ ] 2s silence â†’ transcription sent
- [ ] Discord voice active â†’ message appears in channel
- [ ] No project â†’ sent to general prompt
- [ ] Hotkey (Ctrl+Shift+L) â†’ toggles listening
- [ ] Works on Windows
- [ ] Works on macOS
- [ ] Works on Linux

---

## Deployment Checklist

### Pre-Release

- [ ] Bundle base.en model (~142MB)
- [ ] Bundle Silero VAD model (~1MB)
- [ ] Include whisper-stream binaries for all platforms
- [ ] Test on Windows, macOS, Linux
- [ ] Document microphone permissions
- [ ] Add error recovery for missing binaries

### electron-builder.yml

```yaml
extraResources:
  - from: 'resources/whisper/bin'
    to: 'whisper/bin'
    filter: ['**/*']
  - from: 'resources/whisper/models'
    to: 'whisper/models'
    filter: ['*.bin']

win:
  target:
    - nsis
  extraFiles:
    - from: 'resources/whisper/bin/whisper-stream.exe'
      to: 'resources/whisper/bin'

mac:
  extraFiles:
    - from: 'resources/whisper/bin/whisper-stream'
      to: 'resources/whisper/bin'

linux:
  extraFiles:
    - from: 'resources/whisper/bin/whisper-stream'
      to: 'resources/whisper/bin'
```

---

## Success Metrics

### Performance

- âœ… Time to first transcription chunk: <300ms
- âœ… Real-time factor: <0.5x (faster than real-time)
- âœ… CPU usage (idle): <5%
- âœ… CPU usage (active): <40%
- âœ… Memory usage: <500MB

### Accuracy

- âœ… Wake word detection: >95% in quiet environment
- âœ… Transcription accuracy: >80% in normal conditions
- âœ… False positive rate: <5%

### UX

- âœ… Button response time: <100ms
- âœ… Visual feedback delay: <50ms
- âœ… End-to-end latency (speak â†’ message): <3s

---

## Risk Mitigation Timeline

| Week | Risk | Mitigation Activity |
|------|------|---------------------|
| Week 1 | Binary build fails | Test on all platforms early |
| Week 2 | Audio capture issues | Test multiple libraries |
| Week 3 | Integration complexity | Keep IPC layer thin |

---

**Timeline is aggressive but achievable. Plan for 2 weeks, expect 3.** ðŸŽ¯
